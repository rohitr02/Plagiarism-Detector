Rohit Rao (rpr79)
Pauli Peralta (pp589)

WFD: 
For the word frequency distribution we used the combination of an array and a linked list. We examined the words in a file one by one and put them in a lexicographically sorted linked list of a word struct, a struct that contained the word itself, its occurence, frequency, and average (the average was not touched in this process). A tokenzier function was created that added each word to the linked list accordingly.
The structure was tested by isolating the tokenizer function as a standalone program and was passed controlled files as arguments for which we knew how many words were present, which ones repeated, and how many times they were repeated. Once the tokenizer function was done tokenizing a file and populating the linked list, we would traverse the linked list again and assign each struct its appropriate frequency value by utilizing a variable that kept track of the total number of words encountered in the file. We then traversed the linked list to ensure that it was indeed in lexicographic order and that each field of the struct was assigned the correct value. We repeated this process with many different files, files with no words, repeated words, only unique words, and a mix of unique and repeated words until we felt that every case was accounted for. If the tokenizer produced a linked list that was not empty, we then created a wfd_node struct, a struct that contains the file name, a pointer to the lexicographically sorted linked list, and the overall word count. This struct was then added an array of wfd_node, the WFD, where we held the appropriate information for every other file that was analyzed. 

JSD:
The WFD populated by the tokenizer was then used to create another array that stored the jsd value. For n files, we knew that we'd be making n(n-1)/2 comaparisons, in other words, for n files, there are going to be n(n-1)/2 jsd values. We took this information and created an array of size n(n-1)/2 and prepopulated it with every pair combination by using the the file names and linked lists stored in WFD. The new prepopulated array consisted of jsdvals structs, structs that held 2 file names, a linked list of words in the respective file, 2 fields that indicated how many words were present in the respective file, and a jsd value field initially set to 0. Once the array was populated, we'd iterate through the array and then iterate through each linked list at each struct. Since the linked list are lexicographically sorted, we traversed them at the same time and compared the words accordingly. At this point we made use of the average field present in the word struct that the linked list is made of and populated the field accordingly. At the end of running through each linked list, we'd have a linked list for each file that now contained each word in the file, the number of times it ocurred in the file, its frequency, and now the average of the word between the current two files being analyzed. We then ran through the linked list again and calculated the KLD using the averages we had just found and then finally calculated the jsd between the two files.
This part was tested with assitance of the tokenizer described above. We isolated the jsd function as a standalone program that took two files as an argument and returned the jsd value. The files were passed to the tokenizer which returned a linked list and then the jsd function would go through the process described above. We tested files that were identical, files that had no words in common, files that had both unqiue and common words, and on empty files. We used files that were relatively short so that we could easily track what the output was supposed to be. 
After going through each element in the array that stored the jsd values, we then used insertion sort to sort thee structs in descending order according to the sum of total words file in each file that the struct contained. 
